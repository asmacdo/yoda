\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{enumitem} % Required for itemize options in normative table
\usepackage{amssymb} % Required for \square symbol

% Scientific Data: max 110 chars, no colons or parentheses, capitalize only first word + proper nouns
\title{STAMPED properties for reproducible research objects}
\author{Austin Macdonald, Cody C. Baker, Yaroslav Halchenko, Isaac To}
\date{March 2025}

\begin{document}

\maketitle

\section{Abstract}

% TODO: compose later when other stuff filled out

\section{Introduction}

Managing data in modern scientific research presents unprecedented challenges due to rapidly expanding dataset scale, complexity, and interdependency.
The FAIR (Findable, Accessible, Interoperable, Reusable) principles [cite] and FAIR4RS guidelines [cite] for research software provide structured guidance to ensure that digital objects are managed in ways that support reuse and reproducibility.
Here we adopt the term \textbf{research object} to be a collection of data, code, and metadata that together represent a complete unit of research output.
Reproducibility of a research object's results requires specific versions of data, code, and computational environments, as well as detailed provenance of how they were used together.
Currently it is most common for these aspects to be managed separately; this separation introduces complexity and undermines reproducibility, transparency, efficiency, and reliability.

The Workflow Community Initiative’s FAIR Workflows Working Group (WCI-FW) [cite] defines a \textbf{workflow} as ``the formal specification of the data flow and execution control between executable components, the expected datasets, and parameter files.’’ % TODO: cite WCI-FW 2025 https://doi.org/10.1038/s41597-025-04451-9
WCI-FW systematically mapped existing FAIR and FAIR4RS principles onto computational workflows, defining metrics for workflow FAIRness and recommendations for workflow developers and systems. % TODO: cite FAIR4RS
However, they explicitly chose not to define new principles for workflows, instead treating them as hybrid data/software objects and applying established FAIR guidelines directly.
FAIR addresses findability, accessibility, interoperability, and reuse of digital objects, but does not address the operational properties---self-containment, provenance tracking, portability---that make a research object’s results actually reproducible.

Enhanced rigor, reproducibility, re-usability, efficiency, and translation across domains are all aspirational goals.
Many researchers already employ practices toward these goals, but lack a common framework to evaluate or communicate their approach.
Here we identify and codify these practices as the STAMPED properties (Self-contained, Tracked, Actionable, Modular, Portable, Ephemeral, and Distributable), originating from the YODA Principles % TODO: cite YODA
, which characterize a well-formed research object.
STAMPED takes a pragmatic approach toward these goals in which each property is a spectrum, from practical minimums---often what researchers are already doing---to aspirational ideals.
STAMPED complements formal computational workflow guidelines (e.g., FAIR4RS, RO-Crate) and facilitates the transition toward comprehensive workflow automation.

% TODO: work term definitions (workflow, module, component, provenance) into Introduction — see proposals/stamped-acronym.md
% Recycled prose:
% A \textbf{workflow} is a sequence of computational steps that transform data within the research object using code components to produce output data.
% A research object is composed of \textbf{modules}---separately distributable collections of components (e.g., an input dataset, a container image, an analysis pipeline).
% A \textbf{component} is a trackable element that is part of a module (e.g., a data file, a script, a configuration file, a metadata file); a module contains components, but not every component is itself a module.
% \textbf{Provenance} is the recorded history of how components were produced or modified, including what actions were performed, what inputs were used, and the versions of everything involved.



\section{Comments}

\begin{table}[ht]
\centering
\caption{The STAMPED Properties. Each property addresses a distinct dimension of research object organization that contributes to reproducibility. Together, they provide a framework for evaluating and improving the reproducibility of computational research.}
\label{tab:stamped}
\begin{tabular}{c l p{10cm}}
\hline
\textbf{Letter} & \textbf{Principle} & \textbf{Description} \\
\hline
S & Self-containment & A research object is a complete retrieval unit---it can be obtained and understood in its entirety without needing to reference external resources. \\
T & Tracking & The provenance of all components is recorded. \\
A & Actionability & Procedures within a research object can be carried out by following or executing its contents. This ranges from well-documented manual steps to fully automated workflows. \\
M & Modularity & All modules are independent and composable. \\
P & Portability & Procedures can be executed on different host environments, given documented system requirements. \\
E & Ephemerality & is able to perform all computation within a throwaway environment. \\
D & Distributability & all modules and components are shareable in a persistent state. \\
\hline
\end{tabular}
\end{table}

The items containing keywords “MUST”, “SHALL”, “SHOULD”, and “MAY” in are to be interpreted as described in RFC 2119. % TODO: cite RFC



\subsection{Self-contained}

\begin{itemize}
    \item \textbf{S.1}: All modules and components essential to replicate computational execution MUST be contained within a single top-level research object
\end{itemize}

Reproducibility fails when a research object depends on resources that are not part of it---an undocumented library, a dataset referenced by a broken URL, a script that assumes tools are installed on the host system.
We call this the ``don't look up'' rule: a research object must never rely on implicit external state.
Instead, it must be a complete retrieval unit, where all modules and components essential to reproduce its results are contained within a single top-level boundary (S.1, Table~1).

Components may be included literally (e.g., files committed directly) or by reference (e.g., as subdatasets, registered data URLs).
Both approaches satisfy self-containment, provided the references are explicit and part of the research object.
At a minimum, everything needed is gathered under one root and any external dependency is explicitly documented---ensuring that nothing is implicitly assumed about the host system, a concern further addressed by Portability.
At the ideal end, every reference is precise enough that retrieval is unambiguous; how to achieve that precision is the concern of Tracking (T) and Distributability (D).

Self-containment is the foundational property upon which the remaining STAMPED properties build.
Tracking, Modularity, Portability, and Distributability each address a different aspect of how that boundary is organized, versioned, and shared---but none are meaningful without first establishing what is inside it.



\subsection{Tracked}

\begin{itemize}
    \item \textbf{T.1}: Version information MUST be recorded for all components
    \item \textbf{T.2}: All components SHOULD be tracked using the same content-addressed version control system
    \item \textbf{T.3}: Provenance of all modifications MUST be recorded
    \item \textbf{T.4}: Code-driven provenance SHOULD be recorded programmatically and MUST include the versions of all components involved
\end{itemize}

Research data frequently exists outside the scope of established version control practices, leading to a multitude of reproducibility issues.
Non-version-controlled data creates ambiguity, as precise identification of dataset states is challenging, and historical changes are difficult to reconstruct or verify.
This ambiguity undermines trust, complicates collaboration, and impairs reproducibility, particularly when datasets undergo frequent updates or transformations.

Version control systems (VCS), such as git, directly address these issues by associating each revision with a unique identifier and recording basic provenance information, including who made the changes, when, and why.
Although commonly called ``version control systems,'' the primary value is not version numbering (``v1'' vs ``v2'') but \emph{content-addressed identification}.
Git describes itself as ``the stupid content tracker'' (from \texttt{man git}), using cryptographic checksums as content identifiers.
Explicit version tags (v1.0, v2.3) are convenience labels atop this content-tracking foundation.
This distinction matters: reproducibility requires precise content identification, not just semantic versioning.
Two datasets labeled ``version 1.0'' by different labs are ambiguous; two datasets with identical content hashes are provably identical.
Throughout the manuscript we will refer to such content tracking systems as VCS to use terminology familiar to the reader, although some initiatives such as \cite{sciops-paper} would describe such requirement as data consistency.

By applying VCS systematically to data and analysis outputs, researchers can pinpoint exact states of digital files, reliably reverting or exploring previous versions as needed.
Although traditional version control systems have limitations when managing large datasets due to storage inefficiencies, modern tools such as git-annex, DataLad, or DVC effectively extend git functionality to handle large, binary, and distributed datasets.
These tools enable researchers to manage extensive datasets without losing the benefits of traditional version control, including improved collaboration through explicit tracking of changes, efficient reversion of undesired modifications, and the ability to reconstruct past research states—effectively enabling "time travel."

Critically, approaching full reproducibility requires that version control encompasses all components essential for replicating computational workflows.
These components typically include raw and derived datasets, preprocessing and analysis scripts, parameter and configuration files, computational environment definitions (such as Docker or Singularity containers), and accompanying documentation describing workflow execution details.
Omitting even one component can severely compromise reproducibility, making comprehensive version control across these assets a fundamental necessity.

% TODO: formal principles removed — now in STAMPED table (S.1, T.1, T.2)
% TODO: strip S-redundant content (lines 155-157 overlap with S section)

Tracking encompasses not only version history but also provenance---the record of what actions produced or modified each component.
Provenance collection typically occurs separately from version control, leading to fragmentation: the data is versioned in one system while the record of how it was produced lives in another (or nowhere).
Embedding provenance directly into version control history---for example, by encoding descriptions of computational actions into commit records---unifies these concerns.
A provenance-rich commit history records not just that a file changed, but what command was executed, what inputs it consumed, and what versions of code and environment were involved.

For code-driven modifications, provenance should be captured programmatically rather than by manual annotation.
This ensures completeness and enables re-execution: a researcher can not only see what was done but repeat it.
% TODO: cite Tek Raj paper, provenance format approaches
% TODO: mention W3C PROV, RO-Crate, BEP028 as interoperable export targets
% TODO: spectrum — minimum (manual commit messages describing what was done) vs ideal (programmatic provenance capture with full dependency graphs, re-executable)
% that's where might be worth already to talk about ability of the used VCS/platform to "replicate" the outside components within it.
% e.g. if containers are used, it must provide storage for those containers as part of the resource, and not to rely on external links which might disappear.
% To a degree - VCS should provide ways to "cauterize the wound" (or find a better analogie).
% It should make it "safe to make a cut", but also to "re-connect" the component if any detail or functionality of it is needed.
% And that should lead into modularity + git modules as the way "to cauterize"



\subsection{Actionable}

\begin{itemize}
    \item \textbf{A.1}: Research object MUST contain sufficient instructions to reproduce all computational results
    \item \textbf{A.2}: Procedures SHOULD be specified as executable specifications
\end{itemize}

A research object may be self-contained and fully tracked, yet having all ingredients and a record of what was done is insufficient without a recipe that can be followed.
Requirement A.1 is what makes a research object operationally useful---the bridge between having the components and being able to use them.

At a minimum, a research object must contain documentation thorough enough for another researcher to follow every step to reproduce results.
Actionability applies to every other STAMPED dimension: at the ideal end, each property is enacted not through documentation alone but through executable specifications (A.2):
\begin{itemize}
  \item \textbf{Self-containment} is more actionable when retrieval of all components is specified and executable (e.g., \texttt{git clone}, \texttt{datalad install}), not just listed in a manifest.
  \item \textbf{Tracking} is more actionable when provenance records are re-executable (e.g., \texttt{datalad rerun}), not just inspectable.
  \item \textbf{Modularity} is more actionable when components can be composed and decomposed via tooling (e.g., \texttt{git submodule}), not just organized into directories.
  \item \textbf{Portability} is more actionable when environments can be instantiated from a specification (e.g., \texttt{singularity run}), not just documented in a README.
  \item \textbf{Ephemerality} is more actionable when computation can be orchestrated in disposable environments (e.g., \texttt{docker compose}, Slurm), not just instructed to ``run in a clean environment.''
  \item \textbf{Distributability} is more actionable when a frozen state can be produced and retrieved by others (e.g., containers, \texttt{npm ci}), not just described with pinning instructions.
\end{itemize}
The principle is tool-agnostic: any system that moves a property from documented to executable moves the research object further along the actionability spectrum.
With advent of agentic AI systems, documentation alone often can provide actionable recipes, but nevertheless formalization in AI-specific formulations (e.g. \texttt{Claude Skills} [CITE]) can drastically improve actionability of such descriptions.



\subsection{Modularity}

\begin{itemize}
    \item \textbf{M.1}: Components SHOULD be organized in a modular structure
    \item \textbf{M.2}: Components MAY be included directly or linked as subdatasets
\end{itemize}

Monolithic structures present significant barriers to effective development, maintenance, and reuse.
Managing such research objects as single, large units complicates incremental updates, obscures dependencies, remixing of components, and can be notably time-consuming.
So rather than managing the research object as one indivisible whole, STAMPED promotes a compositional approach,structuring projects as assemblies of independently versioned and well-defined components.
Defining a module as a separately distributable collection of components for our research object, **Modularity** is achieved when all modules of the research are independent and composable.
Individual parts—such as input and reference datasets, processing scripts, and computational environments—can be updated or replaced independently, thus minimizing disruption and maximizing reusability.

The layout of these components plays a crucial role in enabling effective modularity:

\begin{itemize}
  \item \textbf{code/}: containing analysis scripts and tests
  \item \textbf{inputs/}: encapsulating raw or intermediate datasets
  \item \textbf{envs/}: defining computational environments (e.g., Singularity images)
  \item \textbf{docs/}: for project documentation
  \item \textbf{results/}: explicitly identified outputs (e.g., figures, tables, derived datasets)
\end{itemize}

The intention of such organization clarifies how researchers to compose and recombine modules flexibly to produce varied outcomes.

The spectrum of modularity ranges from:
At \textbf{minimum}, organize components into logical subdirectories---separate code from data, distinguish inputs from results, and document structure in a README.
At a \textbf{middle} level, formalize module boundaries using version control repository boundaries, with major components as separate repositories referencing each other via submodules or documented URLs with version tags.
At the \textbf{ideal} level, implement fully actionable composition with all dependencies as versioned submodules with automated module installation, allowing the entire workflow to be reassembled from a self-contained specification.

Modularity ought to rely heavily on Tracking (module boundaries require independent versioning) and
Self-containment (all independent modules required by the workflow ).
It enhances Actionability by increasing the system reliability.
Portability and Distributability are also easier to achieve since modules can themselves be ported and distributed separately from the integration in the whole.



\subsection{Portability}

\begin{itemize}
    \item \textbf{P.1}: Procedures MUST NOT depend on undocumented host environment state
    \item \textbf{P.2}: Computational environments MUST be explicitly specified
    \item \textbf{P.3}: Environment definitions MUST be version controlled
\end{itemize}

A research object that is self-contained, tracked, and modular may still fail to reproduce if it depends on undocumented host environment state---hardcoded paths, implicitly available tools, or specific OS configurations.
Portability requires that procedures can be executed on different host environments, given documented system requirements.

Research reproducibility fundamentally depends not only on preserving code and data, but also on capturing the complete computational environment—the operating system, libraries, dependencies, and tool versions—that produced the results.
Environmental drift, where software dependencies evolve or disappear over time, is a primary cause of computational irreproducibility. Even with identical code and data, differences in library versions, compiler settings, or system configurations can produce divergent results or outright failures.

Computational environments should be explicitly defined (not implicitly assumed), machine-reproducible, version controlled alongside code and data, and self-contained within the project boundary.

Two primary approaches exist for environment portability:
\begin{enumerate}
  \item \textbf{Container-based}: Docker, Singularity/Apptainer provide OS-level isolation
  \item \textbf{Package-based}: Nix, Guix provide declarative, reproducible package management
\end{enumerate}

STAMPED is environment-mechanism-agnostic; what matters is that
environments are explicitly specified, versioned, and available within the project.

% TODO: containers need more coverage here — containerization is a key portability mechanism that E and D both depend on
% TODO: spectrum — minimum (documented system requirements, no hardcoded paths) vs ideal (machine-reproducible environment specs, containers)
% TODO: the code/ vs code/local/ pattern for non-portable config — see #34
% TODO: randomness/seeds as a portability concern — see #34
% TODO: provenance format targets (W3C PROV, BEP028, RO-Crate, BioComputeObject) — where do these go? Discussion?



\subsection{Ephemeral}

\begin{itemize}
    \item \textbf{E.1}: Computational results SHOULD be produced in ephemeral environments
\end{itemize}

A research object may already have its computational environment documented and versioned,
and a researcher may reproduce it by manually reinstalling the same operating system,
software, dependencies, and configurations at the matching versions, among other
steps---satisfying S, T, and P without Ephemerality.
Such manual reconstitution is not only inconvenient, but may also be imprecise:
environment specifications that cannot be easily tested for reconstitution in different
host systems can be subtly incomplete---for instance, through hidden dependencies on the
researcher's own system that are not apparent until someone else attempts to reproduce
the work.
Ephemerality (E.1, Table~1) addresses this by requiring the computational environment
to be captured in a machine-readable specification from which an equivalent,
host-independent environment can be automatically reconstituted on any compatible platform.

Ephemerality has a reinforcing relationship with the other STAMPED properties:
\begin{itemize}
  \item \textbf{Self-containment (S):} The ephemeral environment must be reconstituted from scratch
    on every run---often by automated tooling---so all necessary specifications must reside
    within the research object itself.
  \item \textbf{Tracking (T):} The exact ephemeral computational context becomes a reproducible
    record alongside code and data.
  \item \textbf{Actionability (A):} The ephemeral computational environment can be easily
    reconstituted.
  \item \textbf{Portability (P):} An ephemeral computational environment often imposes a
    minimum set of requirements on the host system.
  \item \textbf{Distributability (D):} Ephemeral environment specifications---container
    images, reproducible package manifests---are natural units of distribution, packaging
    a frozen computational state that others can retrieve and run.
\end{itemize}

% TODO: Actionability cross-cut — E is more actionable when there is tooling that can orchestrate the entire process ephemerally (docker compose, AWS, Slurm), not just a documented instruction to "run in a clean environment"
If a research object can produce its results in a temporary, disposable environment built solely from its own contents, this provides strong evidence that its other STAMPED properties hold in practice.
Inputs must be exhaustively specified (S), outputs must be deposited correctly (T), and nothing outside the boundary can be relied upon.
Ephemerality is a form of validation: ``make it a habit to destroy the environment.''

Ephemeral computation does not validate everything---for instance, it does not catch reliance on system-specific tools present in the disposable environment itself.
Nevertheless, it substantially raises confidence that the research object is self-contained and portable.

Beyond validation, ephemeral environments enable scaling.
When each computational job runs in an independent, disposable instance, work can be parallelized across subjects, parameters, or datasets---the pattern demonstrated by the FAIRly big workflow approach.
Platforms such as Code Ocean operationalize this by treating each capsule as an ephemeral work unit.

Ephemerality is recommended: not every research object can achieve it, and it is not required for STAMPED compliance.
At a minimum, a research object should be able to produce results from a fresh clone on a system that meets its stated requirements (P).
At the ideal end, every computation runs in a disposable environment that is created and destroyed per execution.



\subsection{Distributable}

\begin{itemize}
    \item \textbf{D.1}: All referenced modules and components MUST be persistently retrievable by others
    \item \textbf{D.2}: Environment specifications SHOULD support reproducible builds
\end{itemize}

% TODO: Actionability cross-cut — D is more actionable when there is tooling that can distribute a frozen state (containers, PyInstaller, npm ci), not just instructions to "pin your dependencies"
Self-containment (S) establishes that everything needed is within the research object's boundary, possibly by reference.
Distributability promises that those references actually deliver---that the research object and its components can be shared, retrieved, and used by others in a state consistent with reuse.

The distinction mirrors the concept of a software distribution: a curated, versioned bundle in which all components are resolved to specific versions and packaged for consumption.
Simply sharing a research object---uploading scripts to a repository with loose dependencies, or posting files on a website with no version guarantees---does not constitute distribution in this sense.
A distributable research object is packaged so that it can be retrieved in the same state as intended

Distributability also has a circular relationship with the other STAMPED properties: someone else's distribution effort often serves as the starting point for a new research object's self-containment.
Researchers routinely begin by downloading containers, fetching published datasets, and installing released software---modularly composing the distributions of others to assemble their own self-contained research objects.

The spectrum of distributability ranges from publicly accessible components with retrieval instructions, through persistent hosting on archival infrastructure (Zenodo, PyPI, conda-forge, DANDI) with frozen versions and content-addressed identifiers---analogous to moving from a development checkout (\texttt{pip install -e .}) to a pinned release (\texttt{pip freeze})---to an ideal in which all external dependencies are mirrored or the entire research object is packaged as a self-contained archive (e.g., a built container or a zipped RO-Crate).

\begin{table}[ht]
\centering
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Principle} & \textbf{Requirements} \\
\hline
\textbf{To be Self-contained:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item all modules and components needed to understand and execute the Research Object are retrievable as a single unit.
  \item external dependencies are explicitly documented with retrieval instructions.
  \item there are no implicit references to undocumented external resources.
\end{itemize} \\
\hline
\textbf{To be Tracked:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item every component has version information (commit hash, tag, or identifier).
  \item changes to components are recorded with timestamps and authorship.
  \item provenance records capture the computational history, context, and transformations.
\end{itemize} \\
\hline
\textbf{To be Actionable:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item instructions for executing procedures are present and unambiguous.
  \item execution paths can be followed manually or automated programmatically.
  \item the Research Object transitions from documentation to operational capability.
\end{itemize} \\
\hline
\textbf{To be Modular:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item modules can be independently modified.
  \item components are organized in logical, separable units.
  \item modules can be composed together or used in isolation.
\end{itemize} \\
\hline
\textbf{To be Portable:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item system requirements and dependencies are explicitly documented.
  \item the Research Object is flexible enough to execute on different host environments without modification by the user.
  \item environment specifications are machine-readable where possible.
\end{itemize} \\
\hline
\textbf{To be Ephemeral:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item computation can occur in temporary, disposable environments.
  \item results are reproducible without knowledge of previous runs.
  \item no reliance on external configurations or host system states (such as OS registry modifications).
\end{itemize} \\
\hline
\textbf{To be Distributable:} &
\begin{itemize}[noitemsep,topsep=0pt,leftmargin=*]
  \item all modules and components can be shared in a persistent, retrievable state.
  \item dependencies are frozen or pinned to specific versions across systems.
  \item the Research Object can be obtained by others in the same state as intended.
\end{itemize} \\
\hline
\end{tabular}
\caption{Normative statements about STAMPED properties of research objects.}
\label{tab:stamped-principles}
\end{table}





\subsection{Checklist for compliance to principles}

We hope that the preceding sections have provided a clear understanding of the STAMPED properties and their rationale.
While this understanding is essential, researchers may also benefit from a practical checklist to assess their research objects against these principles.
This checklist provides such a guide for assessing compliance with STAMPED principles, ordered by the strength of the requirement (MUST, SHOULD, MAY).

\subsubsection*{MUST}
\begin{itemize}[label=$\rightarrow$]
    \item \textbf{Self-containment (S.1)}: All modules and components essential to replicate computational execution are contained within a single top-level research object.
    \begin{itemize}[label=$\square$]
        \item Are all files and directories nested under a common root?
        \item Are datasets included, linked, or referenced adjacent to the code and environment specifications?
        \item Is external software included in the environment or linked as submodules within the project boundary?
    \end{itemize}
    \item \textbf{Tracking (T.1)}: Version information is tracked for all components.
    \begin{itemize}[label=$\square$]
        \item Is Git used for code, text, documentation, and configuration files?
        \item Are tools like git-annex, DataLad, or Git LFS used for large binary data?
    \end{itemize}
    \item \textbf{Actionability (A.1)}: The research object contains sufficient instructions to reproduce all computational results.
    \begin{itemize}[label=$\square$]
        \item Is a \texttt{README.md} or \texttt{Makefile} included with instructions for installation and usage?
        \item Is there a clear entry point for users to start reproducing results (e.g., a main script, a workflow definition, or a container image)?
        \item Is the workflow tested regularly to ensure instructions remain accurate?
    \end{itemize}
    \item \textbf{Portability (P.1)}: Procedures MUST NOT depend on undocumented host environment state (hardcoded paths, implicitly available tools, specific OS configurations).
    \begin{itemize}[label=$\square$]
        \item Are relative paths used in scripts, avoiding hardcoded or system-specific paths like \texttt{C:\textbackslash Users\textbackslash ...} or \texttt{/home/user/...}?
        \item Are all software dependencies included in the environment specification, rather than relying on pre-installed tools?
        \item Are there any assumptions about the host system's configuration that are not documented (e.g., specific OS versions, required system libraries, or environment variables)?
    \end{itemize}
    \item \textbf{Portability (P.2)}: Computational environments are explicitly specified.
    \begin{itemize}[label=$\square$]
        \item Is there a clear list of system requirements and dependencies documented in the README or environment specifications?
        \item Are there tests to verify that the workflow can run on a clean system that meets the stated requirements?
    \end{itemize}
    \item \textbf{Portability (P.3)}: Environment definitions are version controlled.
    \begin{itemize}[label=$\square$]
        \item Are environment specifications (e.g., Dockerfiles, Singularity definitions, Nix expressions) included in the version control system alongside code and data?
        \item Is there a process for updating environment specifications when dependencies change, and are these updates tracked in version control?
        \item Are environment specifications tested to ensure they can be reconstituted as intended?
    \end{itemize}
    \item \textbf{Distributability (D.1)}: Computational environments are explicitly specified.
    \begin{itemize}[label=$\square$]
        \item Are exact environment specifications (e.g., container digests, frozen package manifests) included to ensure others can attempt to replicate the specific environment?
        \item Are environment specifications shared in a way that others can access and use them (e.g., published container images, archived environment files)?
        \item Is there documentation on how to obtain and use the environment specifications for reproduction?
    \end{itemize}
    \item \textbf{Distributability (D.3)}: Environment definitions are version controlled.
    \begin{itemize}[label=$\square$]
        \item Are the exact environment specifications used to generate a specific set of results included in the version control system alongside code and data?
        \item Is there a process for updating environment specifications when dependencies change, and are these updates tracked in version control?
    \end{itemize}
\end{itemize}

\subsubsection*{SHOULD}
\begin{itemize}[label=$\rightarrow$]
    \item \textbf{Tracking (T.2)}: All components are tracked using the same content-addressed version control system.
    \begin{itemize}[label=$\square$]
        \item Is only one of Git, DVC, git-annex or DataLad used across all components?
    \end{itemize}
    \item \textbf{Modularity (M.1)}: Components are organized in a modular structure.
    \begin{itemize}[label=$\square$]
        \item Are raw data, processed data, code, and environment definitions separated into distinct modules?
    \end{itemize}
    \item \textbf{Ephemerality (E.1)}: Computational results are computed in ephemeral environments.
    \begin{itemize}[label=$\square$]
        \item Is the pipeline tested in a fresh container, batch job, clean virtual machine, or cloud-based instance?
        \item Do scripts clean up temporary files and avoid reliance on pre-existing local state?
    \end{itemize}
    \item \textbf{Distributability (D.2)}: Environment specifications support reproducible builds.
    \begin{itemize}[label=$\square$]
        \item Are the exact environment specifications used to generate a specific set of results regularly tested to ensure they can be reconstituted as intended?
        \item Are the exact environment specifications used to generate a specific set of results included in the provenance records to link computational actions to specific environments?
    \end{itemize}
    \item \textbf{Distributability (D.4)}: Environment components are self-contained within the dataset.
    \begin{itemize}[label=$\square$]
        \item Are dependencies or container layers archived within the research object where possible (e.g., using Singularity SIF files, npm package-lock.json files)?
    \end{itemize}
\end{itemize}

\subsubsection*{MAY}
\begin{itemize}[label=$\rightarrow$]
    \item \textbf{Modularity (M.2)}: Components are included directly or linked as subdatasets.
    \begin{itemize}[label=$\square$]
        \item Are external datasets or software dependencies included as submodules or linked as submodules?
        \item Is the modular structure documented to clarify how components relate and can be recombined?
        \item Are modular boundaries defined in a way that allows independent updates without breaking the overall workflow?
        \item Is there a clear mechanism for composing modules together (e.g., git submodules, or container orchestration)?
        \item Is there documentation or tooling to support users in understanding and navigating the modular structure?
    \end{itemize}
\end{itemize}





\subsection{Enabling tools}
% TODO: tools before examples to provide more context/understanding
% TODO: increases interoperability among tools

STAMPED properties are complementary, not competitive, with existing standards:
\begin{itemize}
    \item \textbf{STAMPED + BIDS}: Domain standard within STAMPED structure
    \item \textbf{STAMPED + RO-Crate}: Organization during research, packaging for publication
    \item \textbf{STAMPED + FAIR}: Structure projects, then share via FAIR
    \item \textbf{STAMPED + workflow systems}: Organize, then execute (Nextflow, Snakemake, CWL)
    \item \textbf{STAMPED + platforms}: Download, version, and compose locally
\end{itemize}

\subsubsection{git-annex Complexity vs Git LFS Simplicity}
Achieving STAMPED compliance with git-annex offers maximum flexibility, supporting many remote types including SSH, S3, HTTP, local drives, and even offline USB drives.
This comes at a cost: a steeper learning curve.
Git LFS offers a simpler alternative that is transparent to users and has native GitHub/GitLab support with broad adoption.
The trade-off: Git LFS is centralized (requires an LFS server), lacks offline capability, and has less flexible remote configurations.
Lightweight STAMPED implementations using Git LFS are feasible for easier onboarding, sacrificing federation flexibility.

\subsubsection{When STAMPED Is vs Is Not Appropriate}

\textbf{STAMPED excels when}:
\begin{itemize}
    \item Multi-institutional collaboration requires federated datasets
    \item Long-term reproducibility spans decades or requires offline archives
    \item Heterogeneous workflows are not tied to a specific platform
    \item Data sovereignty requirements preclude mandatory cloud upload
    \item Complex dependency graphs involve subdatasets from diverse sources
\end{itemize}

\textbf{STAMPED may be more than needed for}:
\begin{itemize}
    \item Single-script analyses where version control alone suffices
    \item Ephemeral exploratory work such as Jupyter notebooks
    \item Cloud-native ML production where Pachyderm or a platform may be a better fit
    \item Small teams with simple workflows where DVC or Git LFS may suffice
\end{itemize}

\subsection{Examples and Case Studies}
% TODO: point to living documentation, minimal examples/counter-examples
% TODO: FAIRly big workflow, AIND, non-neuroscience examples?

Several research projects have already adopted and documented YODA principles---the predecessor to STAMPED---demonstrating practical utility across domains.

\textbf{BIDS Standard Evolution}: A significant validation of the ``do not look up'' principle occurred within the Brain Imaging Data Structure (BIDS) community.
Originally, the BIDS specification nested derived data under \texttt{derivatives/} within the original raw dataset, creating upward dependencies that violated portability.
Following YODA principles, the BIDS community reversed this relationship: derivative datasets now exist independently and reference raw data as subdatasets, not vice versa.

This architectural shift influenced major neuroimaging tools:
\begin{itemize}
    \item \textbf{fMRIPrep}: Switched default output layout to follow YODA structure
    % TODO: add GitHub issue reference for "yoda" layout proposal
    \item \textbf{OpenNeuroDerivatives}: Entire derivative dataset collection now follows YODA organization, separating processed outputs from raw data dependencies
    \item \textbf{BIDS specification}: Updated to accommodate and recommend YODA-compliant layouts for derivative datasets
\end{itemize}

\textbf{Workflow Platforms}: Infrastructure projects implementing YODA at scale:
\begin{itemize}
    \item \textbf{BABS}: Platform implementing the ``FAIRly big workflow'' approach, demonstrating YODA principles for large-scale neuroimaging analyses with containerized pipelines and modular dataset composition
    \item \textbf{CRCNS.org}: Neuroscience data sharing platform providing YODA-structured templates and validation tools
\end{itemize}

These adoptions demonstrate that STAMPED properties solve practical organizational challenges across scales, from individual studies to community-wide infrastructure.

\subsection{Lit Review}
% TODO: restructure as comparison to related approaches, not tool-by-tool breakdown

% TODO: tie in the idea of how lots of specific tools have inherently followed these principles, but how STAMPED  formalizes them and provides a framework for any workflow, not just specific tools, can gain the same benefits.

The challenges STAMPED addresses are not unique to neuroscience or even scientific computing broadly.
Multiple communities independently developed organizational frameworks exhibiting remarkable convergent evolution toward similar core patterns.

Between 2003 and 2025, at least 19 distinct initiatives emerged addressing research data organization, version control, and reproducibility:

% TODO: add references to all!
\textbf{Foundational principles}: Noble's 2009 guidelines for computational biology organization, FAIR principles (2016) for data sharing, Software Carpentry's ``Good Enough Practices'' (2017).

\textbf{Version control extensions}: git-annex (2010), Git LFS (2015), DVC (2017), Pachyderm (2014), Quilt (2020s) all independently arrived at ``Git for data'' concepts, applying proven version control semantics to large datasets.

\textbf{Cloud platforms}: Code Ocean (2017), brainlife.io (2017), Flywheel (2018), Galaxy (2005) provide turnkey reproducibility services, all implementing code/environment/data trinity separation.

\textbf{Framework tooling}: Kedro (2019), nipoppy (2023), Cookiecutter Data Science (2016) provide opinionated structures for data engineering and neuroimaging workflows.

% Should remove BEP028- too niche etc, but might be worth refer to the PROV itself! and potentially BIDS itself since provides poor man versioning (CHANGES) + structure + linking to SourceDatasets
\textbf{Metadata standards}: RO-Crate (2019), BioComputeObject (IEEE 2791-2020), PROV (TODO), BIDS (2016) address packaging and provenance standardization.

Despite independent origins, these frameworks converged on core patterns:
\begin{itemize}
    \item \textbf{Separation of concerns}: Data, code, environment, and results managed distinctly
    \item \textbf{Immutability}: Raw data never modified in-place
    \item \textbf{Hierarchical organization}: Nested, modular structures
    \item \textbf{Version control}: Applying Git concepts beyond just code
    \item \textbf{Provenance tracking}: Recording how outputs were generated
\end{itemize}

% TODO: this block is tool-specific (git-annex, DataLad) not STAMPED-general. Reframe or move to Discussion.
STAMPED's unique contributions within this landscape:
\begin{itemize}
    \item \textbf{Federated composition}: Subdatasets can live anywhere (not centralized)
    \item \textbf{Interface agnosticism}: Works with any container/pipeline system
    \item \textbf{Local-first design}: Works offline, no cloud platform dependency
    \item \textbf{git-annex flexibility}: Many remote types vs single-server models
    \item \textbf{Scale via modularity}: Proven from single files to 8000+ subdatasets (datasets.datalad.org)
\end{itemize}

This convergent evolution validates that STAMPED properties are not arbitrary choices but responses to fundamental challenges in computational research reproducibility.
The formalization presented here provides a foundation for interoperability and comparison across this evolving ecosystem of tools and standards.

\subsubsection{DVC (Data Version Control)}
Shares ``Git for data'' philosophy but differs in architecture:
DVC uses external .dvc files with remote storage configuration, while tools complementary to STAMPED such as DataLad with git-annex integrate directly into the git repository structure.
DVC focuses on Python and ML workflows within a single project, whereas STAMPED is language-agnostic and supports federated multi-project composition.
Both are valid; choice depends on team familiarity (DVC easier for ML engineers), infrastructure (DVC integrates easily with cloud ML platforms), and scale requirements (git-annex more flexible for federated datasets).

\subsubsection{Pachyderm}
Enterprise ``Git for data'' with Kubernetes-native architecture.
Pachyderm requires a centralized cluster and cloud infrastructure, while a STAMPED approach is local-first, works offline, and supports federation.
Pachyderm provides automatic pipeline triggering, whereas STAMPED emphasizes explicit, provenance-tracked execution (e.g., \texttt{datalad run}).
Pachyderm excels for production ML pipelines, team collaboration with shared compute resources, and automatic scaling.
STAMPED excels for research workflows, offline work, and heterogeneous datasets across institutions.

\subsubsection{Kedro}
Python data engineering framework with modular pipelines.
Kedro provides within-project modularity through Python pipeline components, while STAMPED provides across-project modularity through subdatasets as git submodules.
Kedro includes built-in visualization (Kedro-Viz), whereas STAMPED is CLI-focused with integration to external tools.
These approaches are complementary: a Kedro pipeline inside a STAMPED research object combines both strengths.

\subsubsection{Cloud Platforms (Code Ocean, brainlife.io, Flywheel)}
Turnkey reproducibility services with proprietary interfaces.
Platforms offer zero local setup and institutional partnerships; STAMPED offers local control with no vendor lock-in.
Platforms provide web GUIs with point-and-click interaction; STAMPED provides CLI/API with scriptable automation.
Platforms are centralized (single capsule or project); STAMPED is federated (compose across repositories).
STAMPED research objects could wrap platform outputs: download results, organize locally with full versioning, and compose across platforms via subdatasets.

\subsection{Future Directions}

\subsubsection{Provenance Standards Convergence}
Ongoing work toward unified provenance representation, including BEP028 and RO-Crate convergence, will improve STAMPED interoperability with domain-specific tools.

\subsubsection{Tool Certification}
Formal STAMPED properties enable ``STAMPED-compliant'' tool certification, similar to BIDS validation.

\subsubsection{Teaching Materials}
Operationalized principles provide a foundation for structured curriculum development beyond current tutorial-based approaches.
\section{Data Availability}
\section{Code Availability}
\section{References}

% TODO: Might be worth relating to "Ten simple rules for principled simulation modelling"
% https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009917
% "Rule 5: Be kind to your future self" — STAMPED assists in accessing all necessary components and history.

\section{Author Contributions}

\section{Competing Interests}
The authors declare no competing interests.

\section{Acknowledgments}

\section{Funding}
This work was supported by the National Institutes of Health:
ReproNim --- Center for Reproducible Neuroimaging Computation (NIBIB P41~EB019936),
OpenNeuro --- An Open Archive for Analysis and Sharing of BRAIN Initiative Data (NIMH R24~MH117179),
DANDI --- Distributed Archives for Neurophysiology Data Integration (NIMH R24~MH117295),
and EMBER --- Ecosystem for Multi-modal Brain-behavior Experimentation and Research (NIMH R24~MH136632).

\end{document}
